{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries\n"
      ],
      "metadata": {
        "id": "93YF9yoyxiBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate bert_score sacrebleu rouge_score numpy"
      ],
      "metadata": {
        "id": "boeuOfXNeT3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XYv8D6KUxHKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Load the model and tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HgoMk8mNxQk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"\")"
      ],
      "metadata": {
        "id": "Jp6SojzG4frO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"user/model_name\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "KjGmw6V4xdV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the test dataset"
      ],
      "metadata": {
        "id": "jWsr3Kcix5eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"user/dataset\"\n",
        "dataset = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "79Yn1jERx8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "ZX0-qfxxyE3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change this part according to your own dataset format.\n",
        "input_texts = [conv['value'] for data in dataset['test'] for conv in data['conversations'] if conv['from'] == 'human']\n",
        "references = [[conv['value']] for data in dataset['test'] for conv in data['conversations'] if conv['from'] == 'gpt']\n"
      ],
      "metadata": {
        "id": "RKuT880pyGe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate predictions"
      ],
      "metadata": {
        "id": "7lVSnW57yLrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model, tokenizer, input_texts, max_length=256):\n",
        "    predictions = []\n",
        "    for text in input_texts:\n",
        "        inputs = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "        outputs = model.generate(inputs, max_length=max_length)\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "ZU5pw3d4yPtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = generate_predictions(model, tokenizer, input_texts)"
      ],
      "metadata": {
        "id": "U_p37OUZyV0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize evaluation metrics\n"
      ],
      "metadata": {
        "id": "xUuE5bLbyYlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "SVvWzOFoybR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute BertScore\n"
      ],
      "metadata": {
        "id": "F6NvylycHunb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore_results = bertscore.compute(predictions=predictions, references=[ref[0] for ref in references], lang=\"en\")"
      ],
      "metadata": {
        "id": "ub2ViYsPHxn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Bleu score"
      ],
      "metadata": {
        "id": "7T11ukAbH0kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_results = bleu.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "xTKZa567H0Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computer Rouge-1, Rouge-2, Rouge-L"
      ],
      "metadata": {
        "id": "dILbXPVfQkV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_results = rouge.compute(predictions=predictions, references=[ref[0] for ref in references])"
      ],
      "metadata": {
        "id": "rB-CFcoxWNHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_precision = np.mean(bertscore_results['precision'])\n",
        "avg_recall = np.mean(bertscore_results['recall'])\n",
        "avg_f1 = np.mean(bertscore_results['f1'])\n",
        "\n",
        "# Print results\n",
        "print(f\"BERTScore - Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}\")\n",
        "print(f\"BLEU Score: {bleu_results['score']:.4f}\")\n",
        "print(\"ROUGE-1:\", rouge_results['rouge1'])\n",
        "print(\"ROUGE-2:\", rouge_results['rouge2'])\n",
        "print(\"ROUGE-L:\", rouge_results['rougeL'])\n"
      ],
      "metadata": {
        "id": "40LKPNK6WPnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the inference results to an Excel file"
      ],
      "metadata": {
        "id": "wo3t67UOydmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-vRdCHUxygyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Human Prompt': input_texts,\n",
        "    'Generated Response': predictions,\n",
        "    'Reference Response': references\n",
        "})\n",
        "\n",
        "# Ensure the directory exists\n",
        "path = '/content/drive/My Drive/'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "# Save the DataFrame to an Excel file in the specified directory\n",
        "file_path = os.path.join(path, 'excel_file.xlsx')\n",
        "df.to_excel(file_path, index=False)\n",
        "\n",
        "print(f'File saved to {file_path}')\n"
      ],
      "metadata": {
        "id": "ujj_1xow6eX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}